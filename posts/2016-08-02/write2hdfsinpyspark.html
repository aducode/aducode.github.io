<!DOCTYPE html>
<html>
  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="chrome=1">

    <link rel="stylesheet" type="text/css" href="../../stylesheets/stylesheet.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/github-dark.css" media="screen">
    <link rel="stylesheet" type="text/css" href="../../stylesheets/prism.css" media="screen">
	<script src="../../javascripts/statistics.js"></script>
    <title>在pyspark中操作hdfs文件</title>
  </head>

  <body>

    <header>
      <div class="container">
        <h1>在pyspark中操作hdfs文件</h1>
        <h2>Raven&#39;s Blog</h2>

        <section id="downloads">
		  <a href="http://aducode.github.io/" class="btn btn-star" target="_blank"><span class="icon"></span>Home Page</a>
          <a href="https://github.com/aducode" class="btn btn-github" target="_blank"><span class="icon"></span>View on GitHub</a>
		  <a href="mailto:aducode@126.com" class="btn btn-email"><span class="icon"></span>Send Email</a>
        </section>
      </div>
    </header>

    <div class="container">
      <section id="main_content">
        <h3>
<a id="在pyspark中操作hdfs文件" class="anchor" href="#%E6%AC%A2%E8%BF%8E%E6%9D%A5%E5%88%B0issac%E7%9A%84%E5%8D%9A%E5%AE%A2" aria-hidden="true"><span class="octicon octicon-link"></span></a>在pyspark中操作hdfs文件</h3>
		<h2>背景</h2>

<p>这段时间的工作主要是跟spark打交道，最近遇到类似这样的需求，统计一些数据（统计结果很小），然后直接把文本数据写入到hdfs文件中去。</p>

<p>之前一直使用的是scala语言，实现起来非常简单：</p>

<pre class="language-scala line-numbers">
<code class="language-scala">
import org.apache.spark.SparkContext
import org.apache.hadoop.fs.{FileSystem, Path}

def path(filepath:String) =
  new Path(filepath)

def getFileSystem(sc:SparkContex) =
  FileSystem.get(sc.hadoopConfiguration)

def write(sc:SparkContext, filepath:String, content:String, overwrite:Boolean = true):Unit = {
  try{
    val fileSystem = getFileSystem(sc)
    val fileName = path(filepath)
    val out = fileSystem.create(fileName, overwrite)
    out.write(content.getBytes)
    out.flush
    out.close
  }catch{
    case e:Exception => System.err.println(e.toString)
  }
}</code>
</pre>

<p>但是scala那陡峭的学习曲线，并不适合整个团队的发展，和业务的快速迭代，所以我们统一改成了spark的python接口：pyspark</p>

<p>那么pyspark中该如何直接操作hdfs上的文件呢？
找了一圈，并没有在python的SparkContext中找到hadoopConfiguration()方法，开始用了一些比较ugly的方法，比如在python中直接调用hadoop命令去操作文件。</p>

<p>后来通过看pyspark的代码，发现使用的是 <strong>py4j</strong> 来连接python与java，根据py4j的原理，可以通过以下代码在python中调用java对象来操作hdfs文件:</p>

<pre class="language-python line-numbers">
<code class="language-python">
#!/usr/bin/python
# -*- coding:utf-8 -*-

def path(sc, filepath):
  """
  创建hadoop path对象
  :param sc sparkContext对象
  :param filename 文件绝对路径
  :return org.apache.hadoop.fs.Path对象
  """
  path_class = sc._gateway.jvm.org.apache.hadoop.fs.path
  return path_class(filepath)

def get_file_system(sc):
  """
  创建FileSystem
  :param sc SparkContext
  :return FileSystem对象
  """
  filesystem_class = sc._gateway.jvm.org.apache.hadoop.fs.FilFileSystem
  hadoop_configuration = sc._jsc.hadoopConfiguration()
  return filesystem_class.get(hadoop_configuration)

def write(sc, filepath, content, overwite=True):
  """
  写内容到hdfs文件
  :param sc SparkContext
  :param filepath 绝对路径
  :param content 文件内容
  :param overwrite 是否覆盖
  """
  try:
    filesystem = get_file_system(sc)
    out = filesystem.create(path(sc, filepath), overwrite)
    out.write(bytearray(content, "utf-8"))
    out.flush()
    out.close()
  except Error, e:
    print e
</code>
</pre>

<p>那么这个神奇的py4j是什么呢？</p>

<p>可以看下官网的介绍：<a href="http://www.py4j.org/">http://www.py4j.org/</a></p>

<p>这里大概说下个人理解：
要想在java和python中使用这个东西，需要在java代码中开一个本地socket监听端口，并暴露一个entrypoint，python代码中通过JavaGateway来跟java进程交互，也就是说，这个是一个跨语言的RPC调用</p>

      </section>
	  <section style="font-size:13px;color:gray;">
	  <div style="float:left">联系作者:<a href="mailto:aducode@126">aducode@126.com</a></div>
	  <div style="float:right">更多精彩文章请点击:<a href="http://aducode.github.io">http://aducode.github.io</a></div>
	  </section>
    </div>
	<script src="../../javascripts/prism.js"></script>
  </body>
</html>

